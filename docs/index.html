<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Corentin Dancette</title>
<meta name="description" content="Corentin Dancette personal website and blog
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- Open Graph -->


  </head>

  <body class=" sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm sticky-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center">
  <a href="mailto:%63%6F%6E%74%61%63%74@%63%64%61%6E%63%65%74%74%65.%66%72"><i class="fas fa-envelope"></i></a>
  
  <a href="https://scholar.google.com/citations?user=2zReQdQAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
  
  
  <a href="https://github.com/cdancette" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
  <a href="https://www.linkedin.com/in/cdancette" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
  <a href="https://twitter.com/cdancette" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>
  
  
  
  
</span>

        </div>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          <ul class="navbar-nav ml-auto flex-nowrap">
            <!-- About -->
            <li class="nav-item active">
              <a class="nav-link" href="/#publications">
                publications
                
                  <span class="sr-only">(current)</span>
                
              </a>
            </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    
    <h1 class="post-title"><span class="font-weight-bold">Corentin</span> Dancette</h1>
    <p class="post-description">PhD student at <a href="#">Sorbone Université</a>. Paris, France</p>
  </header>

  <article>
    <div class="row">
      <div class="col">
        <p>Hi! I am a french computer science student, I gratuated from Ecole Centrale Paris, France, and Georgia Institute of Technology, USA.</p>

<p>I am currently a PhD student at the <a href="https://mlia.lip6.fr/">MLIA</a> team, at the LIP6 lab, in Sorbone Université, Paris. I am mainly working on visual reasoning tasks, such as Visual Question Answering.</p>

<p>I was previously intern at the <a href="http://www.lscp.net/persons/dupoux/bootphon/index.html">Cognitive Machine Learning team</a> at Ecole Normale Supérieure, in Paris, as a machine learning research intern.</p>

<p>I’m very interested in machine learning and deep learning, more specifically for robotics, computer vision, and speech processing, and its application for AI research.</p>

<h2 id="education">Education</h2>

<p><strong>Master degree, Computer Science, Georgia Institute of Technology</strong>
I graduated from Georgia Tech, with a specialization in machine learning and interactive intelligence.</p>

<p><strong>Engineering student, Ecole Centrale Paris, France</strong>
I studied Engineering and Computer Science at Ecole Centrale Paris.</p>

<h2 id="internships-and-projects">Internships and projects</h2>

<p><strong>Research Internship, CoML, Spring 2018</strong><br />
I interned for 6 months at the <a href="http://www.lscp.net/persons/dupoux/bootphon/index.html">CoML team</a>, a research lab in École Normale Supérieure in Paris. I worked on unsupervised machine learning for words and phoneme discovery
in speech data.</p>

<p>I contributed to the package ABNet3, a siamese neural network for speech embedding. The package is available on github: <a href="https://github.com/bootphon/abnet3">https://github.com/bootphon/abnet3</a></p>

<p><strong>Study of the Variational Auto Encoder for speech subword modeling</strong><br />
For a class project, at Georgia Tech, I studied a variational Auto Encoder architecture for the Zerospeech Challenge, Track 1: unsupervised subword modeling. You can find my work here: <a href="https://cdancette.fr/zerospeech-vae/">https://cdancette.fr/zerospeech-vae/</a></p>

<p><strong>Deep learning for optical flow estimation, Fall 2017</strong><br />
As a graduate student at Georgia Tech, I worked on a project to estimate optical flows in a natural environment dataset. 
I used the FlowNet2 architecture. You can find my code on <a href="https://github.com/cdancette/flownet-tools">github</a></p>

<p><strong>Software engineering internship, Datadog, Fall 2016</strong><br />
I worked in the data engineering team, managing data pipelines with Spark and Hadoop.</p>

      </div>
        
        <div class="profile col-4 ">
          
            <img class="img-fluid z-depth-1 rounded" src="/assets/profile.jpg">
          
          
        </div>
        
    </div>

    

    
  </article>
</div>

<hr>
<a class="anchor" id="publications"></a>
<h1>Publications</h1>
<div class="publications">
  
    <h2 class="year">2020</h2>
    <ol class="bibliography"><li><!-- 
  Which fields are displayed ?

  - abbr: short abstract ?
  - type
  - title
  - author
  - abstract
  - arxiv
  - html
  - pdf
  - suppl
  - poster
  - slides
  - code

 -->


<div class="row">
  <div class="col-sm-2">
  
    <img class="img-fluid" src=/assets/bibliography/scn.png>
  
  <!-- 
    
    <abbr class="badge">In review</abbr>
    
   -->
  </div>

  <div id="dancette2020overcoming" class="col-sm-8">
    
      <span class="title">Overcoming Statistical Shortcuts for Open-ended Visual Counting</span>
      <span class="author">
        
          
            
              
                <em>Corentin Dancette</em>,
              
            
          
        
          
            
              
                
                  <a href="http://remicadene.com" target="_blank">Remi Cadene</a>,
                
              
            
          
        
          
            
              
                
                  Xinlei Chen,
                
              
            
          
        
          
            
              
                
                  and <a href="https://webia.lip6.fr/~cord/" target="_blank">Matthieu Cord</a>
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>arXiv preprint arXiv:2006.10079</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a class="bibtex">BibTex</a>]
    
    
      [<a href="http://arxiv.org/abs/2006.10079" target="_blank">arXiv</a>]
    
    
    
      <!-- [<a href="/assets/pdf/https://arxiv.org/pdf/2006.10079.pdf" target="_blank">PDF</a>] -->
      [<a href="https://arxiv.org/pdf/2006.10079.pdf" target="_blank">PDF</a>]
      
    
    
    
    
      [<a href="https://github.com/cdancette/spatial-counting-network" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Machine learning models tend to over-rely on statistical shortcuts. These spurious correlations between parts of the input and the output labels does not hold in real-world settings. We target this issue on the recent open-ended visual counting task which is well suited to study statistical shortcuts. We aim to develop models that learn a proper mechanism of counting regardless of the output label. First, we propose the Modifying Count Distribution (MCD) protocol, which penalizes models that over-rely on statistical shortcuts. It is based on pairs of training and testing sets that do not follow the same count label distribution such as the odd-even sets. Intuitively, models that have learned a proper mechanism of counting on odd numbers should perform well on even numbers. Secondly, we introduce the Spatial Counting Network (SCN), which is dedicated to visual analysis and counting based on natural language questions. Our model selects relevant image regions, scores them with fusion and self-attention mechanisms, and provides a final counting score. We apply our protocol on the recent dataset, TallyQA, and show superior performances compared to state-of-the-art models. We also demonstrate the ability of our model to select the correct instances to count in the image. Code and datasets are available: this https URL </p>
    </span>
    
    
    <span class="bibtex hidden">
      <pre>@article{dancette2020overcoming,
  title = {Overcoming Statistical Shortcuts for Open-ended Visual Counting},
  author = {Dancette, Corentin and Cadene, Remi and Chen, Xinlei and Cord, Matthieu},
  journal = {arXiv preprint arXiv:2006.10079},
  year = {2020},
  arxiv = {2006.10079},
  pdf = {https://arxiv.org/pdf/2006.10079.pdf}
}
</pre>
    </span>
    
  </div>
</div>
</li></ol>
  
    <h2 class="year">2019</h2>
    <ol class="bibliography"><li><!-- 
  Which fields are displayed ?

  - abbr: short abstract ?
  - type
  - title
  - author
  - abstract
  - arxiv
  - html
  - pdf
  - suppl
  - poster
  - slides
  - code

 -->


<div class="row">
  <div class="col-sm-2">
  
    <img class="img-fluid" src=/assets/bibliography/rubi.png>
  
  <!-- 
    
    <abbr class="badge">NeurIPS</abbr>
    
   -->
  </div>

  <div id="cadene2019rubi" class="col-sm-8">
    
      <span class="title">RUBi: Reducing Unimodal Biases for Visual Question Answering</span>
      <span class="author">
        
          
            
              
                
                  <a href="http://remicadene.com" target="_blank">Remi Cadene</a>,
                
              
            
          
        
          
            
              
                <em>Corentin Dancette</em>,
              
            
          
        
          
            
              
                
                  <a href="https://webia.lip6.fr/~cord/" target="_blank">Matthieu Cord</a>,
                
              
            
          
        
          
            
              
                
                  Devi Parikh,
                
              
            
          
        
          
            
              
                
                  and  others
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In NeurIPS</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a class="bibtex">BibTex</a>]
    
    
      [<a href="http://arxiv.org/abs/1906.10169" target="_blank">arXiv</a>]
    
    
    
      <!-- [<a href="/assets/pdf/https://arxiv.org/pdf/1906.10169.pdf" target="_blank">PDF</a>] -->
      [<a href="https://arxiv.org/pdf/1906.10169.pdf" target="_blank">PDF</a>]
      
    
    
    
    
      [<a href="https://github.com/cdancette/rubi.bootstrap.pytorch" target="_blank">Code</a>]
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p> Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings.
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. Our code is available: http://github.com/cdancette/rubi.bootstrap.pytorch </p>
    </span>
    
    
    <span class="bibtex hidden">
      <pre>@inproceedings{cadene2019rubi,
  title = {RUBi: Reducing Unimodal Biases for Visual Question Answering},
  author = {Cadene, Remi and Dancette, Corentin and Cord, Matthieu and Parikh, Devi and others},
  booktitle = {NeurIPS},
  pages = {841--852},
  year = {2019},
  arxiv = {1906.10169},
  pdf = {https://arxiv.org/pdf/1906.10169.pdf}
}
</pre>
    </span>
    
  </div>
</div>
</li></ol>
  
    <h2 class="year">2018</h2>
    <ol class="bibliography"><li><!-- 
  Which fields are displayed ?

  - abbr: short abstract ?
  - type
  - title
  - author
  - abstract
  - arxiv
  - html
  - pdf
  - suppl
  - poster
  - slides
  - code

 -->


<div class="row">
  <div class="col-sm-2">
  
  <!--  -->
  </div>

  <div id="thual2018k" class="col-sm-8">
    
      <span class="title">A K-nearest neighbours approach to unsupervised spoken term discovery</span>
      <span class="author">
        
          
            
              
                
                  Alexis Thual,
                
              
            
          
        
          
            
              
                <em>Corentin Dancette</em>,
              
            
          
        
          
            
              
                
                  Julien Karadayi,
                
              
            
          
        
          
            
              
                
                  Juan Benjumea,
                
              
            
          
        
          
            
              
                
                  and Emmanuel Dupoux
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In 2018 IEEE Spoken Language Technology Workshop (SLT)</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a class="bibtex">BibTex</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Unsupervised spoken term discovery is the task of finding recurrent acoustic patterns in speech without any annotations. Current approaches consists of two steps: (1) discovering similar patterns in speech, and (2) partitioning those pairs of acoustic tokens using graph clustering methods. We propose a new approach for the first step. Previous systems used various approximation algorithms to make the search tractable on large amounts of data. Our approach is based on an optimized k-nearest neighbours (KNN) search coupled with a fixed word embedding algorithm. The results show that the KNN algorithm is robust across languages, consistently out-performs the DTW-based baseline, and is competitive with current state-of-the-art spoken term discovery systems.</p>
    </span>
    
    
    <span class="bibtex hidden">
      <pre>@inproceedings{thual2018k,
  title = {A K-nearest neighbours approach to unsupervised spoken term discovery},
  author = {Thual, Alexis and Dancette, Corentin and Karadayi, Julien and Benjumea, Juan and Dupoux, Emmanuel},
  booktitle = {2018 IEEE Spoken Language Technology Workshop (SLT)},
  pages = {491--497},
  year = {2018},
  organization = {IEEE}
}
</pre>
    </span>
    
  </div>
</div>
</li>
<li><!-- 
  Which fields are displayed ?

  - abbr: short abstract ?
  - type
  - title
  - author
  - abstract
  - arxiv
  - html
  - pdf
  - suppl
  - poster
  - slides
  - code

 -->


<div class="row">
  <div class="col-sm-2">
  
  <!--  -->
  </div>

  <div id="Riad2018sampling" class="col-sm-8">
    
      <span class="title">Sampling Strategies in Siamese Networks for Unsupervised Speech Representation Learning</span>
      <span class="author">
        
          
            
              
                
                  Rachid Riad,
                
              
            
          
        
          
            
              
                <em>Corentin Dancette</em>,
              
            
          
        
          
            
              
                
                  Julien Karadayi,
                
              
            
          
        
          
            
              
                
                  Neil Zeghidour,
                
              
            
          
        
          
            
              
                
                  Thomas Schatz,
                
              
            
          
        
          
            
              
                
                  and Emmanuel Dupoux
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proc. Interspeech</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
      [<a class="bibtex">BibTex</a>]
    
    
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Recent studies have investigated siamese network architectures for learning invariant speech representations using same-different side information at the word level. Here we investigate systematically an often ignored component of siamese networks: the sampling procedure (how pairs of same vs. different tokens are selected). We show that sampling strategies taking into account Zipf’s Law, the distribution of speakers and the proportions of same and different pairs of words significantly impact the performance of the network. In particular, we show that word frequency compression improves learning across a large range of variations in the number of training pairs. This effect does not apply to the same extent to the fully unsupervised setting, where the pairs of same-different words are obtained by spoken term discovery. We apply these results to pairs of words discovered using an unsupervised algorithm and show an improvement on the state-of-the-art in unsupervised representation learning using siamese networks. </p>
    </span>
    
    
    <span class="bibtex hidden">
      <pre>@inproceedings{Riad2018sampling,
  author = {Riad, Rachid and Dancette, Corentin and Karadayi, Julien and Zeghidour, Neil and Schatz, Thomas and Dupoux, Emmanuel},
  title = {Sampling Strategies in Siamese Networks for Unsupervised Speech Representation Learning},
  year = {2018},
  booktitle = {Proc. Interspeech},
  pages = {2658--2662},
  doi = {10.21437/Interspeech.2018-2384},
  url = {http://dx.doi.org/10.21437/Interspeech.2018-2384}
}
</pre>
    </span>
    
  </div>
</div>
</li></ol>
  

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2020 Corentin Dancette.
    Powered sort_by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>







</html>
